## TODO
Remember to check all prints Ive added to Litex build, since some of them change stuff (like rom loading)
https://github.com/trustedcomputinggroup/tpm

-- Starting litex
mkdir project-petalite
cd project-petalite
python3 -m venv .venv
source .venv/bin/activate
mkdir litex
cd litex
wget https://raw.githubusercontent.com/enjoy-digital/litex/master/litex_setup.py
chmod +x litex_setup.py
python3 ./litex_setup.py --init --install --config=full

cd ..
pip3 install meson ninja
sudo ./litex/litex_setup.py --gcc=riscv
pip3 install litex/litex -- looks redundant but is necessary
source /tools/Xilinx/Vivado/2024.2/settings64.sh


-- Chipyard:
1. Install conda
2. Run sudo apt-get install linux-image-generic
sudo apt install libguestfs-tools
3. Run installation as per documentation
4. source /home/franos/chipyard/env.sh


-- Vivado
sudo apt-get update
sudo apt-get install locales
sudo locale-gen en_US.UTF-8
sudo update-locale
source /tools/Xilinx/2023.1/Vivado/2023.1/settings64.sh
source /tools/Xilinx/2025.1.1/Vivado/settings64.sh
vivado&

-- If installing older version of Vivado (like 2023.1), also do:
wget http://security.ubuntu.com/ubuntu/pool/universe/n/ncurses/libtinfo5_6.3-2ubuntu0.1_amd64.deb
sudo apt install ./libtinfo5_6.3-2ubuntu0.1_amd64.deb
wget http://security.ubuntu.com/ubuntu/pool/universe/n/ncurses/libncurses5_6.3-2ubuntu0.1_amd64.deb
sudo apt install ./libtinfo5_6.3-2ubuntu0.1_amd64.deb


-- GTKWave
sudo apt install build-essential meson gperf flex desktop-file-utils libgtk-3-dev libbz2-dev libjudy-dev libgirepository1.0-dev
sudo apt install libgtk-4-dev
git clone "https://github.com/gtkwave/gtkwave.git"
cd gtkwave
sudo meson setup build && cd build && meson install


-- Yosis
source /home/franos/projects/project-petalite/tools/oss-cad-suite/environment


-- To run QuestaSim in Vivado:
1. Install the correct version of QuestaSim, which is 2024.1

1. Install compatible version of gcc, which for Questa 2024.1, is 7.4.
    1.1. Read https://chatgpt.com/c/68378209-0244-800a-b94b-61cb9fc717f0

sudo update-alternatives --config gcc

2. Change /home/franos/intelFPGA_pro/24.2/questa_fse/bin/vcom to add:
+elif echo "$@" | grep -q -- "-version" ; then
+  exec "$arg0" "$@" | sed 's@ Intel Starter FPGA Edition@Sim@'
around line 180, right at the end, or so.

3. In Vivado, go to Tools, Compile Simulation libs

4. Wait for compilation (around 1:20h or so)

5. Set:
export PATH=$PATH:/home/franos/intelFPGA_pro/24.2/questa_fse/bin
export LM_LICENSE_FILE=/home/franos/intelFPGA_pro/LR-236507_License.dat:$LM_LICENSE_FILE
::env(LM_LICENSE_FILE)  "/home/franos/intelFPGA_pro/LR-236507_License.dat"
puts $::env(LM_LICENSE_FILE)


-- Verilator
1. Follow tutorial in https://verilator.org/guide/latest/install.html, using Install into a Specific Prefix
2. Then do export PATH="/tools/verilator/v5.036-48-g0dc93c1d5/bin:$PATH"


# Commands
source /tools/Xilinx/2025.1.1/Vivado/settings64.sh
export PATH="/tools/verilator/v5.036-48-g0dc93c1d5/bin:$PATH"

This is a fresh install.
INFO Could not detect the display scale (hDPI).
       If you are using a high resolution monitor, you can set the insaller scale factor like this: 
       export XINSTALLER_SCALE=2
       setenv XINSTALLER_SCALE 2

./scripts/build.sh
./scripts/build.sh --force-all
./soc/main.py --sim --io-json=soc/data/io_sim.json --build-dir=builds/soc --compile-gateware --firmware=builds/firmware.bin --load
OPTIONALLY: --trace --debug-bridge
python test/tpm.py --interactive --log


./scripts/build.sh --board
./scripts/build.sh --force-all --board
./soc/main.py --build-dir=builds/soc --compile-gateware --firmware=builds/firmware.bin &> synth-log.txt
python test/tpm.py --serial --interactive --log

BOARD IS xc7vx690tffg1761-3

# In tcp connection
ssh user@100.100.128.87
scp builds/soc/gateware/digilent_netfpga_sume.bit user@100.100.128.87:~/franos-cm/bitstreams/digilent-netfpga-sume-v3.bit

export PATH=$HOME/franos-cm/python3.12/bin:$PATH
source .venv/bin/activate
source /tools/Xilinx/Vivado/2023.1/settings64.sh

python test/tpm.py --serial --log
./scripts/program-board.sh ~/franos-cm/bitstreams/digilent-netfpga-sume-v2-trng.bit


MORE COMMANDS:
litex_server --udp --udp-ip=192.168.1.50
litex_cli --csr-csv="./builds/soc/csr.csv" --regs
gtkwave ./builds/soc/gateware/sim.fst
./firmware/firmware.py show

-- Adding swap mem to SoC:
sudo swapoff -a
sudo rm /swapfile
sudo fallocate -l 64G /swapfile
sudo chmod 600 /swapfile
sudo mkswap /swapfile
sudo swapon /swapfile
free -h


-- USB access in WSL:
winget install --interactive --exact dorssel.usbipd-win
usbipd list
usbipd bind --busid <busid>
usbipd attach --wsl --busid <busid>
lsusb (in linux)

Also, install Xilinx drivers:
cd <Vivado Install>/data/xicom/cable_drivers/lin64/install_script/install_drivers/
sudo ./install_drivers

cd builds/soc/gateware
vivado&
open_project digilent_netfpga_sume.xpr
open_checkpoint ./digilent_netfpga_sume_route.dcp

# Debugging
15h... got to 0xea... stopped
running for 4h10min, currently in 0c-3
4 cores seem to work really well, 8 is apparently almost exactly the same

0x8 in wolfssl in 25min

We need to implement the shims in tpm_to_platform_interface.h AND ONLY THOSE (as far as I can tell)
There are other _plat funcs, but they are for simulation purposes.
Actually... some of the ones in platform_public_interface need to be CONCEPTUALLY implemented
That is, we might need to have StartUp implemented as deasserting the SoC reset for example.
But the function calls themselves dont seem all that necessary?
They also seem to implement vars (like s_isCanceled) that need to be signals in the SoC itself... we might need some HAL here
About this last topic (s_isCanceled and such), ask Copilot to summarize it for me
Finally, we need to wrap the TPM command with a one (?) byte header, so we can also transmit stuff needed for the side-band (i.e. Cancel request, Locality, _TPM_Hash_Start)
That is, if we still want to use UART, which seems like the safer bet. Otherwise, we would have to deal with
how to share a bus between the TPM and the host in Litex (which might be possible, but still), and that would break our
qemu simulation architecture... so best to stick to UART.

- Depending on the TPM specs, we might need two uarts... one for control signals, the other for data
- That would be needed if the specs say that we should be able to stop an operation midway through, for example


Currently, verilator simulation works, but takes a long time (order of 8 hours)
    - We can run just the tpm, no pqc, on my boards
        - kinda useless since we are already gonna try it in netfpga
    - We can run the tpm, using wolfssl, normally on my host machine, and see how long it takes
        - Useful so we can know if the problem is the wolfssl, or the SoC Core, or the simulation
    - Suppose the wolfssl takes a long time in the host machine, then the problem is the tpm-wolf interaction.
        -Then... is it worth figuring that out?
    - Suppose we cant make the fpga design any faster, and it takes a similar time to the simulation (or lets say, more than 5 min)
        - Then we can have the TPM running on a normal host machine, because we proved that we could have a firmware version of it
            - Then we just do calls to the dilithium on the board.
            - And we can also base our design on the existing FutureTPM
So next steps:
    1. Run tpm with wolfssl on host machine
    2. Run SoC on board, time it.
    3. Figure out from there.


Add a second uart just so we can send control signals?
or drive one uart from two different places? what clock domain should it be in?